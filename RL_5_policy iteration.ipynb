{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1_MmO5cbOak-WI2g7DIdi7Jq7MgnVW1cN","timestamp":1711866327691}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Prasad Jawale RL Experiment 5 - Policy Iteration\n"],"metadata":{"id":"e---7lEIgSLF"}},{"cell_type":"code","source":["import numpy as np\n","\n","class GridWorldMDP:\n","    def __init__(self, num_rows, num_cols, terminal_states, rewards):\n","        self.num_rows = num_rows\n","        self.num_cols = num_cols\n","        self.terminal_states = terminal_states\n","        self.rewards = rewards\n","        self.num_states = num_rows * num_cols\n","        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up\n","        self.transition_prob = self._initialize_transition_prob()\n","\n","    def _initialize_transition_prob(self):\n","        transition_prob = np.zeros((self.num_states, len(self.actions), self.num_states))\n","        for state in range(self.num_states):\n","            for action_index, action in enumerate(self.actions):\n","                next_state = self._get_next_state(state, action)\n","                transition_prob[state, action_index, next_state] = 1\n","        return transition_prob\n","\n","    def _get_next_state(self, state, action):\n","        row, col = divmod(state, self.num_cols)\n","        next_row = max(0, min(row + action[0], self.num_rows - 1))\n","        next_col = max(0, min(col + action[1], self.num_cols - 1))\n","        next_state = next_row * self.num_cols + next_col\n","        if (next_row, next_col) in self.terminal_states:\n","            return next_state, self.rewards[(next_row, next_col)]\n","        return next_state, 0\n","\n","    def policy_evaluation(self, policy, discount_factor=0.9, theta=0.0001):\n","        V = np.zeros(self.num_states)\n","        while True:\n","            delta = 0\n","            for state in range(self.num_states):\n","                v = V[state]\n","                new_v = 0\n","                for action_index, action in enumerate(self.actions):\n","                    next_state, reward = self._get_next_state(state, action)\n","                    new_v += policy[state, action_index] * (reward + discount_factor * V[next_state])\n","                V[state] = new_v\n","                delta = max(delta, abs(v - V[state]))\n","            if delta < theta:\n","                break\n","        return V\n","\n","    def policy_iteration(self, discount_factor=0.9):\n","        policy = np.ones((self.num_states, len(self.actions))) / len(self.actions)\n","        while True:\n","            V = self.policy_evaluation(policy, discount_factor)\n","            policy_stable = True\n","            for state in range(self.num_states):\n","                old_action = np.argmax(policy[state])\n","                action_values = np.zeros(len(self.actions))\n","                for action_index, action in enumerate(self.actions):\n","                    next_state, reward = self._get_next_state(state, action)\n","                    action_values[action_index] = reward + discount_factor * V[next_state]\n","                best_action = np.argmax(action_values)\n","                if old_action != best_action:\n","                    policy_stable = False\n","                policy[state] = np.eye(len(self.actions))[best_action]\n","            if policy_stable:\n","                break\n","        return policy, V\n","\n","\n","\n","\n","num_rows = 5\n","num_cols = 3\n","terminal_states = {(0, 0): 1, (4, 2): 1}\n","rewards = {(0, 0): 1, (4, 2): 1}\n","grid_world = GridWorldMDP(num_rows, num_cols, terminal_states, rewards)\n","\n","# Policy iteration\n","optimal_policy, optimal_values = grid_world.policy_iteration()\n","\n","# Displaying the optimal policy\n","actions_str = ['R', 'L', 'D', 'U']\n","for state in range(grid_world.num_states):\n","    row = state // num_cols\n","    col = state % num_cols\n","    print(f\"State ({row}, {col}): Action {actions_str[np.argmax(optimal_policy[state])]} Optimal Value: {optimal_values[state]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EbIgQqjlyjuV","executionInfo":{"status":"ok","timestamp":1711866370991,"user_tz":-330,"elapsed":8,"user":{"displayName":"PRASAD JAWALE","userId":"08908178032325548227"}},"outputId":"42c59ed7-adff-4506-d7de-7509100609ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["State (0, 0): Action L Optimal Value: 9.999153585021714\n","State (0, 1): Action L Optimal Value: 9.999238226519543\n","State (0, 2): Action L Optimal Value: 8.999314403867588\n","State (1, 0): Action U Optimal Value: 9.999238226519543\n","State (1, 1): Action L Optimal Value: 8.999314403867588\n","State (1, 2): Action L Optimal Value: 8.09938296348083\n","State (2, 0): Action U Optimal Value: 8.999314403867588\n","State (2, 1): Action L Optimal Value: 8.09938296348083\n","State (2, 2): Action D Optimal Value: 8.999153585021714\n","State (3, 0): Action U Optimal Value: 8.09938296348083\n","State (3, 1): Action R Optimal Value: 8.999153585021714\n","State (3, 2): Action D Optimal Value: 9.999153585021714\n","State (4, 0): Action R Optimal Value: 8.999153585021714\n","State (4, 1): Action R Optimal Value: 9.999153585021714\n","State (4, 2): Action R Optimal Value: 9.999153585021714\n"]}]}]}