{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ZLroR100A9deOaQsGun-zdezM32ZJ-2H","timestamp":1711866488567}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Prasad Jawale RL Experiment 6 - Value Iteration\n"],"metadata":{"id":"r3AWZicgUeIS"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LqjmWcvy6Bzr","executionInfo":{"status":"ok","timestamp":1711866552758,"user_tz":-330,"elapsed":9,"user":{"displayName":"PRASAD JAWALE","userId":"08908178032325548227"}},"outputId":"75d5ece1-608a-4028-faf0-bf76db8ac06e"},"outputs":[{"output_type":"stream","name":"stdout","text":["State (0, 0): Action L Optimal Value: 9.999153585021714\n","State (0, 1): Action L Optimal Value: 9.999238226519543\n","State (0, 2): Action L Optimal Value: 8.999314403867588\n","State (1, 0): Action U Optimal Value: 9.999238226519543\n","State (1, 1): Action L Optimal Value: 8.999314403867588\n","State (1, 2): Action L Optimal Value: 8.09938296348083\n","State (2, 0): Action U Optimal Value: 8.999314403867588\n","State (2, 1): Action L Optimal Value: 8.09938296348083\n","State (2, 2): Action D Optimal Value: 8.999165052157794\n","State (3, 0): Action U Optimal Value: 8.09938296348083\n","State (3, 1): Action R Optimal Value: 8.999165052157794\n","State (3, 2): Action D Optimal Value: 9.999165052157794\n","State (4, 0): Action R Optimal Value: 8.999165052157794\n","State (4, 1): Action R Optimal Value: 9.999165052157794\n","State (4, 2): Action R Optimal Value: 9.999165052157794\n"]}],"source":["import numpy as np\n","\n","class GridWorldMDP:\n","    def __init__(self, num_rows, num_cols, terminal_states, rewards, gamma=0.9):\n","        self.num_rows = num_rows\n","        self.num_cols = num_cols\n","        self.terminal_states = terminal_states\n","        self.rewards = rewards\n","        self.gamma = gamma\n","        self.num_states = num_rows * num_cols\n","        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up\n","        self.transition_prob = self._initialize_transition_prob()\n","\n","    def _initialize_transition_prob(self):\n","        transition_prob = np.zeros((self.num_states, len(self.actions), self.num_states))\n","        for state in range(self.num_states):\n","            for action_index, action in enumerate(self.actions):\n","                next_state, _ = self._get_next_state(state, action)\n","                transition_prob[state, action_index, next_state] = 1\n","        return transition_prob\n","\n","    def _get_next_state(self, state, action):\n","        row, col = divmod(state, self.num_cols)\n","        next_row = max(0, min(row + action[0], self.num_rows - 1))\n","        next_col = max(0, min(col + action[1], self.num_cols - 1))\n","        next_state = next_row * self.num_cols + next_col\n","        if (next_row, next_col) in self.terminal_states:\n","            return next_state, self.rewards[(next_row, next_col)]\n","        return next_state, 0\n","\n","    def value_iteration(self, theta=0.0001):\n","        V = np.zeros(self.num_states)\n","        while True:\n","            delta = 0\n","            for state in range(self.num_states):\n","                v = V[state]\n","                action_values = []\n","                for action_index, action in enumerate(self.actions):\n","                    next_state, reward = self._get_next_state(state, action)\n","                    action_values.append(reward + self.gamma * V[next_state])\n","                V[state] = max(action_values)\n","                delta = max(delta, abs(v - V[state]))\n","            if delta < theta:\n","                break\n","\n","        policy = np.zeros((self.num_states, len(self.actions)))\n","        for state in range(self.num_states):\n","            action_values = []\n","            for action_index, action in enumerate(self.actions):\n","                next_state, reward = self._get_next_state(state, action)\n","                action_values.append(reward + self.gamma * V[next_state])\n","            best_action = np.argmax(action_values)\n","            policy[state, best_action] = 1\n","\n","        return policy, V\n","\n","\n","\n","num_rows = 5\n","num_cols = 3\n","terminal_states = {(0, 0): 1, (4, 2): 1}\n","rewards = {(0, 0): 1, (4, 2): 1}\n","grid_world = GridWorldMDP(num_rows, num_cols, terminal_states, rewards)\n","\n","# Value iteration\n","optimal_policy, optimal_values = grid_world.value_iteration()\n","\n","# Displaying the optimal policy\n","actions_str = ['R', 'L', 'D', 'U']\n","for state in range(grid_world.num_states):\n","    row = state // num_cols\n","    col = state % num_cols\n","    print(f\"State ({row}, {col}): Action {actions_str[np.argmax(optimal_policy[state])]} Optimal Value: {optimal_values[state]}\")\n"]}]}