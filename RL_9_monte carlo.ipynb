{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pipkoG7f-nfc"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LXuCVK_I-pmA"},"outputs":[],"source":["class GridWorld:\n","    def __init__(self, size=4):\n","        self.size = size\n","        self.grid = np.zeros((size, size))\n","        self.grid[size-1, size-1] = 1  # Terminal state\n","        self.current_position = (0, 0)\n","\n","    def reset(self):\n","        self.current_position = (0, 0)\n","        return self.current_position\n","\n","    def step(self, action):\n","        if action == 0:  # Up\n","            self.current_position = (max(self.current_position[0] - 1, 0), self.current_position[1])\n","        elif action == 1:  # Down\n","            self.current_position = (min(self.current_position[0] + 1, self.size - 1), self.current_position[1])\n","        elif action == 2:  # Left\n","            self.current_position = (self.current_position[0], max(self.current_position[1] - 1, 0))\n","        elif action == 3:  # Right\n","            self.current_position = (self.current_position[0], min(self.current_position[1] + 1, self.size - 1))\n","\n","        reward = 0\n","        if self.current_position == (self.size - 1, self.size - 1):  # Reached terminal state\n","            reward = 1\n","            done = True\n","        else:\n","            done = False\n","\n","        return self.current_position, reward, done, None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2lwi30P-voH"},"outputs":[],"source":["class MonteCarloAgent:\n","    def __init__(self, num_states, num_actions, gamma=0.9):\n","        self.num_states = num_states\n","        self.num_actions = num_actions\n","        self.gamma = gamma\n","        self.Q = np.zeros((num_states, num_actions))\n","        self.returns_sum = np.zeros((num_states, num_actions))\n","        self.returns_count = np.ones((num_states, num_actions))  # Initialized to 1 to avoid division by zero\n","\n","    def update_q_value(self, episode):\n","        G = 0\n","        for t in reversed(range(len(episode))):\n","            state, action, reward = episode[t]\n","            G = self.gamma * G + reward\n","            if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:  # First visit\n","                self.returns_sum[state, action] += G\n","                self.returns_count[state, action] += 1\n","                self.Q[state, action] = self.returns_sum[state, action] / self.returns_count[state, action]\n","\n","    def get_action(self, state, epsilon):\n","        if np.random.random() < epsilon:\n","            return np.random.choice(self.num_actions)\n","        else:\n","            return np.argmax(self.Q[state])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P31azkz--zsC"},"outputs":[],"source":["class TDAgent:\n","    def __init__(self, num_states, num_actions, alpha=0.1, gamma=0.9):\n","        self.num_states = num_states\n","        self.num_actions = num_actions\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.Q = np.zeros((num_states, num_actions))\n","\n","    def update_q_value(self, state, action, reward, next_state):\n","        old_value = self.Q[state, action]\n","        next_max = np.max(self.Q[next_state])\n","        new_value = old_value + self.alpha * (reward + self.gamma * next_max - old_value)\n","        self.Q[state, action] = new_value\n","\n","    def get_action(self, state, epsilon):\n","        if np.random.random() < epsilon:\n","            return np.random.choice(self.num_actions)\n","        else:\n","            return np.argmax(self.Q[state])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kmz0gQBy-2Dc"},"outputs":[],"source":["# Training the agents\n","num_episodes = 100\n","epsilon = 0.1\n","max_steps_per_episode = 100  # Maximum number of steps per episode\n","\n","env = GridWorld()\n","num_states = env.size ** 2\n","num_actions = 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HmlqF72Y-5nV"},"outputs":[],"source":["agent_mc = MonteCarloAgent(num_states, num_actions)\n","agent_td = TDAgent(num_states, num_actions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"N-8j9r7q_A62"},"outputs":[],"source":["for episode in range(num_episodes):\n","    state = env.reset()\n","    done = False\n","\n","    # Run episode with Monte Carlo agent\n","    episode_mc = []  # Store (state, action, reward) tuples\n","    while not done:\n","        action = agent_mc.get_action(state[0] * env.size + state[1], epsilon)\n","        next_state, reward, done, _ = env.step(action)\n","        episode_mc.append((state[0] * env.size + state[1], action, reward))  # Flatten state\n","        state = next_state\n","    agent_mc.update_q_value(episode_mc)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SxSsEV0V_H37"},"outputs":[],"source":["for episode in range(num_episodes):\n","    state = env.reset()\n","    done = False\n","# Run episode with Temporal-Difference agent\n","    state = env.reset()\n","    done = False\n","    while not done:\n","        action = agent_td.get_action(state[0] * env.size + state[1], epsilon)  # Flatten state\n","        next_state, reward, done, _ = env.step(action)\n","        agent_td.update_q_value(state[0] * env.size + state[1], action, reward, next_state[0] * env.size + next_state[1])\n","        state = next_state"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lcpu3x7i_XH_","outputId":"af53b92e-e73f-4ab6-9c7a-f67b21e36660"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average reward for Monte Carlo agent: 1.0\n","Average reward for Temporal-Difference agent: 1.0\n"]}],"source":["import matplotlib.pyplot as plt\n","\n","def evaluate_agent(agent, env, num_episodes=100):\n","    total_rewards = []\n","    for _ in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","        episode_reward = 0\n","        while not done:\n","            action = agent.get_action(state[0] * env.size + state[1], epsilon=0)  # Greedy action\n","            next_state, reward, done, _ = env.step(action)\n","            episode_reward += reward\n","            state = next_state\n","        total_rewards.append(episode_reward)\n","    avg_reward = np.mean(total_rewards)\n","    return avg_reward\n","\n","# Evaluate agents\n","avg_reward_mc = evaluate_agent(agent_mc, env)\n","avg_reward_td = evaluate_agent(agent_td, env)\n","\n","print(\"Average reward for Monte Carlo agent:\", avg_reward_mc)\n","print(\"Average reward for Temporal-Difference agent:\", avg_reward_td)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nwd9brR-_uBF","outputId":"606f5130-6212-4ab6-fb9a-a6753f6b592e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Monte Carlo agent's Q-values:\n","[[2.03763046e-001 4.59886300e-001 1.44790205e-001 3.31575917e-070]\n"," [2.76313265e-070 4.03026290e-066 3.33543634e-002 3.12467625e-001]\n"," [1.31220000e-001 1.78131150e-001 1.96830000e-001 6.28192385e-001]\n"," [1.82250000e-001 7.28543077e-001 1.31220000e-001 2.19915000e-001]\n"," [1.77147000e-001 2.31563620e-001 2.64083650e-001 5.26953846e-001]\n"," [2.26171442e-001 5.97146851e-001 1.84489824e-001 3.71682954e-001]\n"," [5.61867169e-001 2.03467634e-001 1.10414839e-001 2.02500000e-001]\n"," [1.82250000e-001 8.40029268e-001 9.88131292e-324 4.86000000e-001]\n"," [3.14626473e-001 0.00000000e+000 0.00000000e+000 0.00000000e+000]\n"," [3.39314015e-001 2.07465054e-001 2.98771348e-001 6.95641703e-001]\n"," [4.93331521e-001 6.00000000e-001 3.64500000e-001 8.77935000e-001]\n"," [4.99500000e-001 9.89898990e-001 9.88131292e-324 6.00000000e-001]\n"," [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]\n"," [2.30516726e-001 0.00000000e+000 0.00000000e+000 0.00000000e+000]\n"," [0.00000000e+000 0.00000000e+000 0.00000000e+000 6.66666667e-001]\n"," [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]]\n","Temporal-Difference agent's Q-values:\n","[[5.60746445e-02 3.23018944e-03 3.49715680e-02 5.64751042e-01]\n"," [4.44214419e-02 1.29248303e-03 7.71702202e-02 6.44368174e-01]\n"," [2.16542958e-01 7.91313720e-02 1.85929283e-01 7.24672313e-01]\n"," [1.73875912e-01 8.08625272e-01 1.92364095e-01 7.82040898e-02]\n"," [7.83065041e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [3.81225547e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [3.39457216e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [4.62553003e-02 8.99752769e-01 2.44973909e-02 8.07425657e-02]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [1.10351932e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [8.09755425e-02 9.99973439e-01 2.34540454e-05 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"]}],"source":["# Comparison of Policies\n","print(\"Monte Carlo agent's Q-values:\")\n","print(agent_mc.Q)\n","print(\"Temporal-Difference agent's Q-values:\")\n","print(agent_td.Q)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BrnSZw00_wZo","outputId":"c9714875-9b2d-403c-8815-cfeacdb98d57"},"outputs":[{"name":"stdout","output_type":"stream","text":["Monte Carlo agent's learned state values:\n","[0.4598863  0.31246763 0.62819238 0.72854308 0.52695385 0.59714685\n"," 0.56186717 0.84002927 0.31462647 0.6956417  0.877935   0.98989899\n"," 0.         0.23051673 0.66666667 0.        ]\n","Temporal-Difference agent's learned state values:\n","[0.56475104 0.64436817 0.72467231 0.80862527 0.0783065  0.03812255\n"," 0.33945722 0.89975277 0.         0.         0.01103519 0.99997344\n"," 0.         0.         0.         0.        ]\n"]}],"source":["# Comparison of State Values\n","print(\"Monte Carlo agent's learned state values:\")\n","print(np.max(agent_mc.Q, axis=1))\n","print(\"Temporal-Difference agent's learned state values:\")\n","print(np.max(agent_td.Q, axis=1))\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1dfR6PhjbsHxaz0yAxInNzlzkRp0tgtK6","timestamp":1711866505857}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}